# KnowledgeDistillation-pytorch
Implementation of several KD papers using pytorch.

## Dependencies
- torch>=1.6
- torchvision>=0.7

## TODO
[] Tests for network performance on MNIST
[] FitNet
[] Maxout Network

## Reference
- [FitNets: Hints for Thin Deep Nets](https://arxiv.org/abs/1412.6550)
- [Distilling the Knowledge in a Neural Network](https://arxiv.org/abs/1503.02531)

## Codes from other repos
- [maxout](https://github.com/paniabhisek/maxout)
